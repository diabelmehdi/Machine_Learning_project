{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-80997173f861>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnp_utils\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvolutional\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils as utils\n",
    "from keras.layers import Dropout, Dense, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#object 1\n",
    "s = ['side1-0.5.csv', 'side1-0.8.csv', 'side1-1.2m.csv', 'side1-1.6.csv', 'side2-0.5.csv', 'side2-0.8.csv', 'side2-1.2.csv', 'side2-1.6.csv']\n",
    "\n",
    "c = pd.read_csv('object1/'+s[0])\n",
    "c2 = pd.read_csv('object1/'+s[1])\n",
    "c3 = pd.read_csv('object1/'+s[2])\n",
    "c4 = pd.read_csv('object1/'+s[3])\n",
    "c5 = pd.read_csv('object1/'+s[4])\n",
    "c6 = pd.read_csv('object1/'+s[5])\n",
    "c7 = pd.read_csv('object1/'+s[6])\n",
    "c8 = pd.read_csv('object1/'+s[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#object 2\n",
    "s = ['front-black-1.csv', 'front-black-1.2.csv','front-black0.5.csv','front-yellow-1.2.csv', 'front-yellow-1.csv','front-yellow0.5.csv',\n",
    "     'frontblack-0.8.csv','frontblack-1.5.csv', 'frontblack-1.6.csv', 'frontyellow-0.8.csv','frontyellow-1.6.csv',]\n",
    "\n",
    "o = pd.read_csv('object2/'+s[0])\n",
    "o2 = pd.read_csv('object2/'+s[1])\n",
    "o3 = pd.read_csv('object2/'+s[2])\n",
    "o4 = pd.read_csv('object2/'+s[3])\n",
    "o5 = pd.read_csv('object2/'+s[4])\n",
    "o6 = pd.read_csv('object2/'+s[5])\n",
    "o7 = pd.read_csv('object2/'+s[6])\n",
    "o8 = pd.read_csv('object2/'+s[7])\n",
    "o9 = pd.read_csv('object2/'+s[8])\n",
    "o10 = pd.read_csv('object2/'+s[9])\n",
    "o11 = pd.read_csv('object2/'+s[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepairing training set !!! test_data %20\n",
    "\n",
    "dtr1 = c[c.columns[24:30]] \n",
    "dtr2 = c2[c2.columns[24:30]] \n",
    "dtr3 = c3[c3.columns[24:30]] \n",
    "dtr4 = c4[c4.columns[24:30]] \n",
    "dtr5 = c5[c5.columns[24:30]] \n",
    "dtr6 = c6[c6.columns[24:30]]  \n",
    "dtr7 = c7[c7.columns[24:30]]  \n",
    "dtr8 = c8[c8.columns[24:30]] \n",
    "\n",
    "dt1 = o[o.columns[24:30]]  \n",
    "dt2 = o2[o2.columns[24:30]] \n",
    "dt3 = o3[o3.columns[24:30]] \n",
    "dt4 = o4[o4.columns[24:30]] \n",
    "dt5 = o5[o5.columns[24:30]]  \n",
    "dt6 = o6[o6.columns[24:30]] \n",
    "dt7 = o7[o7.columns[24:30]] \n",
    "dt8 = o8[o8.columns[24:30]] \n",
    "dt9 = o9[o9.columns[24:30]]  \n",
    "dt10 = o10[o10.columns[24:30]] \n",
    "dt11 = o11[o11.columns[24:30]] \n",
    "\n",
    "dy_test = [0]* 48 +[1] * 66\n",
    "dy_test = np.asarray(dy_test)\n",
    "\n",
    "d_test = pd.concat([dtr1, dtr2, dtr3, dtr4, dtr5, dtr6, dtr7, dtr8, dt1, dt2, dt3, dt4, dt5, dt6, dt7, dt8, dt9, dt10, dt11], axis=1, join='inner') \n",
    "dy_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepairing training set !!! Validation_data %20\n",
    "\n",
    "dtr1 = c[c.columns[18:24]] \n",
    "dtr2 = c2[c2.columns[18:24]] \n",
    "dtr3 = c3[c3.columns[18:24]] \n",
    "dtr4 = c4[c4.columns[18:24]] \n",
    "dtr5 = c5[c5.columns[18:24]] \n",
    "dtr6 = c6[c6.columns[18:24]] \n",
    "dtr7 = c7[c7.columns[18:24]] \n",
    "dtr8 = c8[c8.columns[18:24]] \n",
    "\n",
    "dt1 = o[o.columns[18:24]] \n",
    "dt2 = o2[o2.columns[18:24]] \n",
    "dt3 = o3[o3.columns[18:24]] \n",
    "dt4 = o4[o4.columns[18:24]] \n",
    "dt5 = o5[o5.columns[18:24]] \n",
    "dt6 = o6[o6.columns[18:24]] \n",
    "dt7 = o7[o7.columns[18:24]] \n",
    "dt8 = o8[o8.columns[18:24]] \n",
    "dt9 = o9[o9.columns[18:24]] \n",
    "dt10 = o10[o10.columns[18:24]] \n",
    "dt11 = o11[o11.columns[18:24]] \n",
    "\n",
    "dy_val = [0]* 48 +[1] * 66\n",
    "dy_val = np.asarray(dy_val)\n",
    "\n",
    "d_val = pd.concat([dtr1, dtr2, dtr3, dtr4, dtr5, dtr6, dtr7, dtr8, dt1, dt2, dt3, dt4, dt5, dt6, dt7, dt8, dt9, dt10, dt11], axis=1, join='inner') \n",
    "dy_val.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepairing training set !!! train_data %60\n",
    "\n",
    "dtr1 = c[c.columns[:18]] \n",
    "dtr2 = c2[c2.columns[:18]] \n",
    "dtr3 = c3[c3.columns[:18]] \n",
    "dtr4 = c4[c4.columns[:18]] \n",
    "dtr5 = c5[c5.columns[:18]] \n",
    "dtr6 = c6[c6.columns[:18]] \n",
    "dtr7 = c7[c7.columns[:18]] \n",
    "dtr8 = c8[c8.columns[:18]] \n",
    "\n",
    "dt1 = o[o.columns[:18]] \n",
    "dt2 = o2[o2.columns[:18]] \n",
    "dt3 = o3[o3.columns[:18]] \n",
    "dt4 = o4[o4.columns[:18]] \n",
    "dt5 = o5[o5.columns[:18]] \n",
    "dt6 = o6[o6.columns[:18]] \n",
    "dt7 = o7[o7.columns[:18]] \n",
    "dt8 = o8[o8.columns[:18]] \n",
    "dt9 = o9[o9.columns[:18]] \n",
    "dt10 = o10[o10.columns[:18]] \n",
    "dt11 = o11[o11.columns[:18]] \n",
    "\n",
    "dy_tr = [0]* 144 +[1] * 198\n",
    "dy_tr = np.asarray(dy_tr)\n",
    "\n",
    "d_train = pd.concat([dtr1, dtr2, dtr3, dtr4, dtr5, dtr6, dtr7, dtr8, dt1, dt2, dt3, dt4, dt5, dt6, dt7, dt8, dt9, dt10, dt11], axis=1, join='inner') \n",
    "dy_tr.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping DATA X_train\n",
    "\n",
    "X_train = d_train.transpose()\n",
    "X_train = np.array(X_train.values.tolist())\n",
    "X_train = X_train.reshape(342,1, 16384,1)\n",
    "\n",
    "\n",
    "# Reshaping DATA X_validation\n",
    "\n",
    "X_validation = d_val.transpose()\n",
    "X_validation = np.array(X_validation.values.tolist())\n",
    "X_validation = X_validation.reshape(114,1, 16384,1)\n",
    "\n",
    "# Reshaping DATA X_test\n",
    "\n",
    "X_test = d_test.transpose()\n",
    "X_test = np.array(X_test.values.tolist())\n",
    "X_test = X_test.reshape(114,1, 16384,1)\n",
    "\n",
    "# Y train assigning\n",
    "y_train = dy_tr\n",
    "y_train = to_categorical(y_train)\n",
    "\n",
    "# Y validation assigning\n",
    "y_validation = dy_val\n",
    "y_validation = to_categorical(y_validation)\n",
    "\n",
    "# Y test assigning\n",
    "y_test = dy_test\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# 1st conv layer\n",
    "model.add(Conv2D(32, (1, 2), activation='relu', input_shape=(1,16384,1)))\n",
    "model.add(MaxPooling2D((1, 2), strides=(1, 2), padding='same'))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "# 2nd conv layer\n",
    "model.add(keras.layers.Conv2D(32, (1, 2), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((1, 2), strides=(1, 2), padding='same'))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "# 3rd conv layer\n",
    "model.add(keras.layers.Conv2D(32, (1, 2), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((1, 2), strides=(1, 2), padding='same'))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "# flatten output and feed it into dense layer\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(80, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.3))\n",
    "\n",
    "# output layer\n",
    "model.add(keras.layers.Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=optimiser,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_validation, y_validation), batch_size=5, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-d795957175f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mhistory_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mloss_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mval_loss_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "  \n",
    "history_dict = history.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "accuracy = history_dict['accuracy']\n",
    "val_accuracy = history_dict['val_accuracy']\n",
    "  \n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "#\n",
    "# Plot the model accuracy vs Epochs\n",
    "#\n",
    "ax[0].plot(epochs, accuracy, 'bo', label='Training accuracy')\n",
    "ax[0].plot(epochs, val_accuracy, 'r', label='Validation accuracy')\n",
    "ax[0].set_title('Training &amp; Validation Accuracy', fontsize=16)\n",
    "ax[0].set_xlabel('Epochs', fontsize=16)\n",
    "ax[0].set_ylabel('Accuracy', fontsize=16)\n",
    "ax[0].legend()\n",
    "#\n",
    "# Plot the loss vs Epochs\n",
    "#\n",
    "ax[1].plot(epochs, loss_values, 'bx', label='Training loss') \n",
    "ax[1].plot(epochs, val_loss_values, 'r', label='Validation loss')\n",
    "ax[1].set_title('Training &amp; Validation Loss', fontsize=16)\n",
    "ax[1].set_xlabel('Epochs', fontsize=16)\n",
    "ax[1].set_ylabel('Loss', fontsize=16)\n",
    "ax[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "\n",
    "print('test_acc:',test_acc )\n",
    "print('test_loss:',test_loss )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.predict_classes(X_test[0])\n",
    "t = X_test[55].reshape(1,1, 16384, 1)\n",
    "model.predict_classes(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
